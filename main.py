from tools.data_loader import load_dataset, save_to_json
from retriever.retriever import retrieve_top_k
from clustering.roberta import cluster_doc
import json
import numpy



def retrieve_and_cluster(query_abstract, paragraph_num, retireve_num):
    paper_retrieved = retrieve_top_k(query_abstract, k=retireve_num)
    paper_clusters = cluster_doc(paper_retrieved, n=paragraph_num)[0]
    return [[query_abstract] + paper_cluster for paper_cluster in paper_clusters]

# retireve and cluster paper abstracts for a given query abstract. save the result to json file.
# json formamt:[{'aid': aid, 
#                'mid': mid, 
#                'abstract': abstract,
#                'related_work': whole related work section concatenated,
#                'retrieved': retreived paper abstracts separated into paragraphs})]

def gen_retrieved_json_for_eval(data_path, tgt_filepath):
    data = load_dataset(data_path)
    retrieved_for_eval = []
    index = 0
    for item in data[89:100]:
        aid = item['aid']
        mid = item['mid']
        abstract = item['abstract']
        related_work = item['related_work']
        paragraph_num = len(item['ref_abstract'])
        retireve_num = sum([len(ref_abstract) for ref_abstract in item['ref_abstract']])

        paper_retrieved = retrieve_and_cluster(abstract, paragraph_num, retireve_num)
        paper_retrieved_reformat = [' SEP '.join(cluster) for cluster in paper_retrieved]

        retrieved_for_eval.append({'aid': aid, 
                'mid': mid, 
                'abstract': abstract,
                'related_work': related_work,
                'retrieved': paper_retrieved_reformat})
        
        index += 1
        print(index)

    save_to_json(retrieved_for_eval, tgt_filepath)

# generate file from the previous json file, 
# the target file will contain paper related work sections
# separated by \n

def gen_target_file_for_eval(data_path, tgt_filepath):
    data = load_dataset(data_path)
    with open(tgt_filepath, 'wt') as tgt_file:
        for item in data:        
            tgt_file.write(item['related_work'] + '\n')

# generate source file for evaluation by concanating the paragraphs 
# generated by model

def gen_source_file_by_concat(data_path, tgt_filepath, src_filepath):
    lengths = []
    data = load_dataset(data_path)
    for item in data:
        lengths.append(len(item['ref_abstract']))

    lengths = numpy.cumsum(lengths)
    lengths = numpy.insert(lengths, 0, 0)

    generated = [x.rstrip() for x in open(src_filepath).readlines()]

    with open(tgt_filepath, 'wt') as tgt_file:
        for index in range(len(lengths) - 1):
            tgt_file.write(' '.join(generated[lengths[index]:lengths[index+1]]) + '\n')

# gen_source_file_by_concat('dataset/Multi-XScience-eval/val.json.gz', 'concat_actual', 'dbart_val_generations.txt')
# dataset = load_dataset('dataset/Multi-XScience/val.json.gz')
# print(dataset[1])

def con_retr_abstracts_for_generation(dataset_path, tgt_filepath):
    d = load_dataset(dataset_path)
    tgt = open(tgt_filepath, "w")
    for item in d:
        print(item)
        for src in item['retrived']:
            tgt.write(src+'\n')

def con_actu_abstracts_for_generation(dataset_path, tgt_filepath):
    d = load_dataset(dataset_path)
    tgt = open(tgt_filepath, "w")
    for item in d[:100]:
        for para in item['ref_abstract']:
            line = item['abstract']
            for _, a in para.items():
                print(a)
                line = line + ' SEP ' + a['abstract']
            tgt.write(line+'\n') 

# con_actu_abstracts_for_generation('dataset/Multi-XScience-eval/val.json.gz', 'concat_act_source')
gen_source_file_by_concat('dataset/Multi-XScience-eval/val.json.gz', 'PG/eval_gt_out_pg_nc_t', 'PG/gen_gt_out_pg_nc_t')