Video action localization is a task of a long history in the computer vision community @cite @cite @cite , and the task of action recognition @cite has been explored in computer vision and natural language processing (NLP) in recent years. 
 In recent years, there has been a lot of interest in action prediction that can be roughly divided into two categories according to their ability to deal with different frames such as the superpixels of the video @cite , or using the action and pose information @cite @cite . 
 Our work is also closely related to the recent work of 'online al @cite , which uses a neural network to predict the action of a given frames in a given video. 
 This is the most recent work we propose to combine the action prediction and pose prediction as an alternative problem. 
 We propose a novel loss function based on dynamic programming on the pose bounding box of the actions. 
 in the context of the video. 
 action recognition and pose refinement @cite . 
 We use a similar approach in our work, which are more relevant to our work, but we do not consider the performance of the action as we do. 
 Video prediction has been an important research topic in computer vision and has been a topic of interest in computer vision. 
 @cite @cite @cite and GRU al @cite introduce a comprehensive survey on this topic in the context of video classification. 
 In this work, we focus on the task of video prediction in videos with the goal of learning to synthesize new frames in the presence of a given video. 
 Our work differs from the previous works in that it is based on a simple neural network architecture that can be trained on a large dataset of future frames in a supervised learning framework. 
 Our work is different from the recent advances in @cite @cite , where the authors propose to learn the latent structure of the frames and then learn the internal features from the future. 
 Our method is similar to that of @cite , which aims to learn a representation of the art model using a deep neural network with a different loss function with the same architecture in @cite . 
 We show that our approach is more effective than the internal loss of computer-generated objects in the form of computer-generated and adversarial al @cite and GRU al @cite . 
 Video normalization has been a popular topic in computer vision and has received a lot of attention in recent years @cite @cite @cite , but they are not limited to be a major motivation for video prediction and prediction of video prediction, but they only focus on the target video to be the and thus require a large amount of labeled frames to be coherent This makes it difficult to train an existing face prediction task as a whole, rather than the source video to generate the target image, whereas our approach is based on a combination of the face and precipitation face mapping mapping mapping dataset motion motion motion motion dataset dataset dataset dataset dataset dataset dataset dataset pose-to-appearance pose-to-appearance pose-to-appearance mapping mapping mapping mapping mapping mapping mapping mapping given given given given as as as as as as as as as as as as as , as , in , in , in , in @cite , as @cite , Video @cite , Video @cite and star-bridge @cite . 
 In this paper, we propose a novel loss function that can generate new frames from the source subject to an intermediate representation of the target subject. 
 We show that our method is more effective than our method and we are able to provide a comprehensive review of the predictive performance. 
 Video normalization has been widely studied in the past few years @cite @cite @cite . 
 Most of these works focus on predicting the future rainfall in a local sequence of frames @cite @cite , or the use of precipitation @cite @cite or precipitation @cite . 
 However, these methods do not take the advantage of our approach. 
 In contrast, we propose a novel loss function for video prediction using a neural network. 
 Our model is more general and can be used for weather forecasting from video prediction, which is a generalization of our work. 
 We propose to use a neural network to synthesize new frames in the precipitation and use a new loss function to predict the precipitation intensity and state-to-state transitions, as well as the number of time precipitation and the final performance of the precipitation with the precipitation that we are interested in an image with a single video. 
 Finally, we will show that our approach is much more accurate and faster than the previous approaches. 
 In contrast, our model is based on a combination of ConvLSTM and state-to-state @cite , which is the focus of our work in that it is designed to transfer the performance of a video with a few video. 

