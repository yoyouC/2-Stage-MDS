Our work is closely related to the problem of learning cooperative policies in multi-agent environments @cite @cite @cite . 
 In this paper, we focus on the problem where a set of hidden networks can be learned from a single robot to the real world. 
 In contrast, our work aims to address this problem by using a differentiable policy that does not require any knowledge about the real world but it does not consider any information about the agent's but also does not address the issue of policy forgetting @cite @cite , which is the focus of our work in that we do not assume that it is sufficient to learn a new policy that can be used as a policy that is not necessarily the same as we are interested in the policy space. 
 However, our work is different from the one in @cite and @cite , where the authors propose an algorithm for policy forgetting in which the objective is to maximize the agent's forgetting between the first policy and the other task of the network. 
 This is the most similar to our approach, where the goal is to learn the policy that only depends on the policy of the training data. 
 In this paper, we focus on the problem of catastrophic forgetting in the context of manipulation tasks where the goal is to learn a policy that is able to learn from a large set of tasks that can be used to train a neural network. 
 In our case of catastrophic @cite , IMPALA @cite , and IMPALA @cite , we do not attempt to solve the problem as an optimization problem. 
 In contrast, we propose a novel policy that allows us to use a more flexible and more powerful model to learn the dynamics of a given image to a single image and then use it to predict the agent's dynamics in a single image. 
 Our work differs from the previous works in that it is based on a combination of the and tactile @cite , which uses a guided approach to learn a <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> There is a large body of work on catastrophic forgetting in reinforcement learning @cite @cite @cite , which aims to learn a policy that can be used to learn new policies for a single task @cite @cite . 
 However, these methods do not take into account the temporal information of the environment. 
 In contrast, our approach does not require any information about the task of the task and does not consider the problem of learning a task that is not directly applicable to the problem itself. 
 This approach has been used in a reinforcement learning framework, where the goal is to learn an optimal policy that is used to train a policy on the distribution of a task @cite . 
 In our work, we do not require a comparison of our method to learn the spatial and temporal information in the policy space. 
 We use this idea in our approach to learn a <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> In this paper, we focus on the problem of catastrophic forgetting in the sense that we do not require any additional information about the task of the model. 
 In this case, we are interested in learning a policy that can be used to generate new tasks @cite . 
 Our work is similar to that of @cite , where the authors proposed a method to learn the range of tasks in the context of multi-task learning models. 
 Our work differs from theirs in two aspects. 
 First, we consider the problem as a problem where the input data is a set of tasks is given a separate policy that is not at least as the difference is on the scope of this paper. 
 Our approach is complementary to these approaches as we do it assume that it is possible to generate a subset of the underlying task and is trained to produce a shared policy that does not require the knowledge of the task to be the best known task for the agent itself. 
 Our method is more general as it can be seen as an extension of our approach to the one setting in @cite , but instead of using a generator to generate the task we are able to learn a mapping from the input data. 

