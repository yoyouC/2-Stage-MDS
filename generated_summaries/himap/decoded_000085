In this paper, we focus on the recent work of deep and boosting @cite @cite , which proposed to learn a Mahalanobis distance metric for a given set of layers and then use them to compute the margin between them. 
 intercluster al @cite propose a method to learn the margin of a deep neural network using a semidefinite network. 
 @cite , the authors propose to learn an embedding model that is able to learn from a large number of data points. 
 In contrast, our approach is based on a combination of the local and the approaches. 
 However, we are not aware of any prior work in this area. 
 The most relevant to our work is the work of EM1vs1 al @cite , where the loss function is learned from the input data set and the margin is not defined as the loss function. 
 This approach is very similar to the one proposed in @cite @cite @cite . 
 The main difference between these two methods is that they are restricted to each other in the same class and are not able to solve the problem of learning the class of overfitting in a single way, which is the main focus of this paper. 
 In @cite , the authors propose to use a CNN as an encoder to learn a mapping from the input image to the target domain. 
 However, this method is not suitable for imbalanced data, where it is not possible to train a classifier that can be used for training. 
 In this paper, we focus on the problem of classifying imbalanced data. 
 Our work differs from the previous work by @cite , in which a CNN is trained to predict the output of the majority of the minority classes. 
 In contrast, our method is able to handle frequent instances of the imbalanced data. 
 In contrast, we propose a novel loss augmentation method for selecting instances from a single image set. 
 We use the least-squared loss as the encoder to generate gradient vanishing and show that it is possible to achieve state-of-the-art performance on the class of data samples. 
 We also show that our approach is more effective than the @cite , and our method in this paper does not require any additional assumptions on the minority class of the network. 
 In addition, we will show that this method can be applied to the class imbalance problem in @cite . 

