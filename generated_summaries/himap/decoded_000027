In this paper, we focus on the problem of multi-agent multi-agent systems which can be roughly divided into two categories. 
 The first category is based on the idea of curriculum learning @cite , where the goal is to find a sequence of agents that can be represented as a reward function @cite @cite @cite . 
 The main difference between these methods and our work is that we do not require any assumption of the reward function, which is the main focus of this paper. 
 We refer readers to @cite for a more comprehensive survey of this area. 
 In this section we review previous works related to this work. 
 We refer to @cite @cite and @cite for more details. 
 We also discuss the results that are closely related to our work in terms of our model and we show that it is not possible to achieve a good optimal reward for the policy that is not always feasible in the case of multi-agent credit assignment. 
 We show that our approach is more effective than the one we present in this paper, though they do not consider the problem as a problem of the problem in which the agents are replicated In this paper, we focus on the problem where the demonstrator is assumed to be known as the reward function. 
 In @cite , the authors propose a credit algorithm to solve the issue of multi-agent credit gradient descent on the global reward function with a multi-agent policy gradient algorithm that allows for the reward function as an optimization problem. 
 The authors in @cite propose a active learning algorithm based on credit gradient term to estimate the demonstrator for a reward function that can be solved efficiently with respect to the global reward. 
 However, the approach is not suitable for multi-agent reinforcement learning @cite @cite @cite , and it is not clear how our algorithm can be adapted to the multi-agent setting @cite , which assumes that the agent is a policy that is not a @cite @cite . 
 In this work, we consider the problem of recovering the demonstrator between samples from different agents by solving the optimization problem of the agent and query samples from specific agents to query samples in order to reduce the demonstrator reward function. 
 This is a similar problem to our problem, which is also used in @cite @cite in the context of credit assignment. 

