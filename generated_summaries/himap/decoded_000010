Road network extraction has been extensively studied in the past few years @cite @cite @cite . 
 In particular, @cite proposed a method to extract road networks from remote sensing imagery using a probabilistic model based on the shortest geographic distance. 
 City-Scale @cite is a semantic network based method for road network extraction based on a probabilistic graphical model that can be used for road sensing imagery extraction of a remote sensing image with a large number of aesthetic labels and a number of clean and sky @cite . 
 City-Scale @cite and City-Scale @cite are the most popular and most similar to ours in that they do not use residual but they use the shortest of the AVA in order to improve the performance of the road network in the context of the road and the use of AVA and AVA @cite , which is used in @cite and @cite for a long time size of the extraction of the graph, and the extraction of the model and the photographic of the model In this paper, we propose a new framework for road area extraction and show that it is not possible to achieve state-of-the-art performance on a large scale dataset. 
 Road and City-Scale @cite proposed a method to estimate the samples of a node by calculating the distance between the labels of the nodes in the original image and the network using the shortest geographic distance. 
 They used this approach to find the noisy label of the network in the training process by minimizing the distance of the neighborhood of the edge in a given time vector. 
 They also showed that the HIP estimators can be used to improve the performance of the HIP Extraction estimators with the help of HIP and Vigna However, they did not consider the problem of finding the best class of the training set in order to achieve the best results. 
 In this paper, we use a Siamese network to generate clean labels for a large number of classes and then do not require any additional assumptions on the labels in which the number of nodes is not necessarily the same as we will show that it is not necessary for the training of a node network that is not applicable to the open-set network extraction problem in @cite , which is not suitable for our task. 
 In this work, we propose a novel iterative learning framework for training data streams. 
 Road network extraction has been studied in @cite @cite @cite . 
 @cite , the authors present a road network based on the shortest geographic information of the graph in order to find the optimal distance between semantically similar categories. 
 (CRESIv2). 
 @cite used a semantic distance labeling scheme for travel time that is labeled as a number of labels and the number of edges in the graph and the labels of the labels are used to OpenStreetMap the extraction of a graph graph @cite @cite , and they are able to find a good optimal distance for a given number of length time @cite . 
 City-Scale @cite also presented a method that can be viewed as a special kind of Road where the travel time is not just but it is computationally expensive and requires a large amount of labels for each of these labels to be used for each roadway. 
 and thus cannot be directly applied to a large number of time time for the extraction task. 
 City-Scale @cite also uses a similar idea to OpenStreetMap the <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank>
