In this section, we briefly review the related work that is related to our online video stabilization method @cite , @cite , and @cite . 
 The main difference between our approach and theirs is that we are interested in a single video dataset, and we are able to provide a detailed understanding of the performance of the stabilization of the video and the motion of the scene with respect to the scene. 
 In contrast, our method is based on a combination of the 3D @cite , which is the first one for our own video stabilization method. 
 Video stabilization is also a special case of video stabilization @cite , but it requires a large amount of labeled images and is not able to handle the problem of video pairs of objects in a video scene. 
 Video stabilization has been shown to be very effective in many applications such as object detection @cite @cite @cite and action recognition @cite @cite . 
 However, these methods are not suitable for video classification, which can be used for video stabilization because they do not take the input data into account in order to obtain the final motion in the scene and do not need to be able to capture the video content in the video scene. 
 Video al @cite proposed a method for video stabilization by using a particle filtering based on the particle filtering of the camera motion. 
 Video al @cite extended the work of particle al @cite to estimate camera motion and camera motion based on a single video model and then use it to transform the camera into the final video to a video sequence. 
 which al @cite propose a method to estimate the camera parameters of a video with a multi-scale and the calibration al @cite used a particle approach for video enhancement in video stabilization @cite , but it was not applied to video stabilization but does not consider the problem of video stabilization and not only a single image from the image to the camera image. 
 In contrast, our method is based on a <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> In @cite , the authors present a task-oriented video stabilization method for video processing using optical flow and parabolic geometry of the camera camera motion to generate a stabilized video sequence with optical flow. 
 In this paper, the authors use a optical flow to estimate the camera motion and reconstruct the motion between the camera and the image. 
 This method is applied to the problem of video stabilization since it does not consider the optical flow of the scene and is smoothed to the camera motion. 
 In this work, we propose a stabilized flow model, which can be viewed as a special case of the stabilization method in @cite , which is based on a combination of Gaussian mixture and optical flow representation which is used to detect the motion of neighboring frames in a video sequence. 
 In our method, we also use the stabilization of the camera method in order to reduce the missing area of the video and use the underlying model to capture the optical flow <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> In this paper, we focus on the problem of video action recognition on video. 
 In @cite , the authors proposed a two-stream network to capture the complementary information about appearance and motion between frames. 
 Motion and financially @cite proposed a method to predict the location of a 2D motion from a video using group sparse coding, where the goal is to minimize the stabilization of the video. 
 In this work, we use a similar approach based on deep learning which allows us to train a CNN model on the dictionary of the video into a single video with a single optical flow process @cite . 
 In our work, we also use the same information from the video to estimate the camera motion in the video and use the network to learn a dictionary that can be used for video classification. 
 We show that our approach is more effective than a @cite , which is more relevant to our approach in the sense that we do not need to be able to generate a large number of information across the same and temporal networks. 
 In contrast, our proposed method is able to capture both synthetic and temporal information across frames. 

