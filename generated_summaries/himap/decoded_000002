similarity-based al @cite proposed a method to predict the compositionality of a word combination by using word embeddings for word embeddings of word combinations of multiword words. 
 @cite and to and multi-prototype and multi-prototype al @cite train a similarity-based model for word association with the goal of predicting whether a word of the word is represented by a word in a given corpus. 
 In @cite , the authors present an approach for estimating the probability distribution of a speech language and word embeddings. 
 In this paper, we use a string similarity, which is similar to our similarity-based model, as we are interested in the language model and is not related to our work. 
 We focus on this problem and can be seen as a more general and more general model for unseen bigrams and the prediction of unseen word embeddings is not more accurate than our method in the literature. 
 We use this idea in the context of word association as we propose in this paper, however, we do not consider the compositionality in the compositionality and the use of word embeddings to predict whether the compositionality is not necessarily the same as we do. 
 Our work differs from the previous work by similarity-based al @cite , which focuses on the compositionality and multi-prototype disambiguation. 
 The work of @cite is the closest to ours in that we deal with the problem of learning the probability of a word from a given word in a back-off model. 
 We use a similar approach to our work in this section. 
 They focus on the word sense that we want to discover a given sense of the word in the target sense that is used for a given context Our work differs from that of @cite , who use a probabilistic word sense on the sense of the words. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 bigrams bigrams bigrams bigrams bigrams bigrams bigrams bigrams bigrams bigrams bigrams bigrams bigrams bigrams bigrams . 
 We have also shown that the use of word sense is a good distinction for our approach to the best-fitting model disambiguation. 
 model model model model model model model model model model model model disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 disambiguation. 
 . 

