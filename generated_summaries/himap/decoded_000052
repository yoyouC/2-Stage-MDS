In this paper, we focus on the recent work of spectral and construction @cite , which is an extended version of this method. 
 The authors of @cite propose a method to learn a dense representation of the convolutional neural network for various tasks in the form of memory Filter and Correlative @cite show that it is possible to achieve state-of-the-art performance on a small number of training examples. 
 Feature al @cite proposed a regularization method based on dense skip connections to improve the performance of the spectral graph in a deep neural network by minimizing the number of filters in the network @cite . 
 In @cite , the authors propose to use the spectral network to learn the inner structure of the network in order to reduce the size of the output of the network. 
 In this work, we propose a novel regularization method that can be used to learn redundant features in the presence of a large number of parameters. 
 In addition, we show that the CF method is effective in @cite , but it is not clear how to learn the <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank> <blank>
