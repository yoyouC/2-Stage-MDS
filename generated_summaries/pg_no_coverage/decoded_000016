there has been a growing body of work on abstractive sentence summarization @ cite @ cite and abstractive abstractive decisions @ cite .
 in this work , we focus on the influence of historical contextual information and a word-level model , which is similar to our work .
 we use extractive summarization to generate salient information about the summary of neural machine translation @ cite , as well as in the context of cross-sentence paragraphs .
 in @ cite the authors propose to use extractive information to extract factual factual information from conversational resources .
 @ cite propose a bidirectional document encoder ( rnn ) to attend to very long sequences, '' of encoder and decoder .
 in contrast , our model is able to predict salient information from source documents.
 We , which allows us to learn a attend of the summary , which can be expressed as a attention-based model .
 the encoder is used to identify the historical representation of source articles , and then use a bidirectional pos cue to predict the document of the model .
 in the work , @ cite proposed a similar approach to abstractive sentiment classification .
 in their work , they propose a morphological context-aware approach for abstractive abstractive summarization.
 model .
 there has been a growing body of work on word completion @ cite @ cite .
 in this work , we focus on the effects of the minimum set of input words in the phrases , which is the focus of our work .
 in @ cite , the authors propose to use the neural network model to predict the topics of each word in a document .
 @ cite propose a deep model to erasing the topics in the document , and the document is modeled as a sentence representation .
 the positional interactions between phrases in the whole sentence are modeled by summing and encode the input and successive sentences .
 the difference is that the topics are conditioned on the positional information of the sentences .
 in contrast , our work is different from ours , since it does not consider the structure of words in phrases .
 in the context of neural networks , we use multi-layer perceptrons ( lstm ) @ cite to represent the intermediate granularity of words .
 the encoder is fed into a lstms to predict topics in a sentence and then concatenate the word vectors from each word to another sentence .
 our work is also related to the work of @ cite and @ cite .
 in this work , we focus on the following aspects : i ) we are interested in the first two steps : ( 1 ) character encoder and 2 ( 2 ) tokens between words and characters .
 we will compare the results obtained in @ cite @ cite for the handwritten word recognition system , in the context of the encoder-decoder model @ cite , which was futher extended to @ cite as well as in this paper .
 in the present work , the authors show that the new model is able to capture the long-distance dependencies between the input and output words .
 in contrast , our approach is based on the assumption that the sequence of words can be interpreted as a vector encoder and decoder .
 in addition , we are able to achieve the state-of-the-art performance on the sentence encoder .
 in particular , we show that our model can be seen as an extension of the concept of HAN @ cite in which the encoder is fed to a sequence of tokens .
 we use a similar approach to speech recognition .

