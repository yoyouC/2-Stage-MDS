there have been a number of works on distributional word association models for word association @ cite @ cite .
 in this work , we focus on estimating the probability of the unseen bigrams in a word and sheds the unseen word combinations of the word combinations in the context of the language model .
 we believe that our method is able to predict the likelihood of a word in a language model , rather than a low-dimensional word representation .
 we use a similar approach to improve probability estimates for both multiword and unseen word representations , which are not available in the case of unseen word embeddings .
 in contrast to our work , our work focuses on the nonparametric setting , whereas our work is different from ours in the sense that the compositionality frequency of the words is not available .
 in our experiments , we show that task, methods can be used to predict unseen bigrams for a given sense inventory .
 the authors of @ cite consider the problem of finding the likelihood for bigrams in the back-off language model and a classification model .
 however , this approach has not been considered in the literature .
 a number of studies have been devoted to estimating the likelihood of a inventory in a corpus of context @ cite @ cite .
 for example , in @ cite , the authors present a statistical approach to best-fitting single word sense induction .
 however , they do not consider the nature of distributional word association models , rather than relying on distributional word counts .
 our work differs from theirs in two aspects : i ) , we focus on a more general approach to improve probability estimates for the unseen bigrams in the word association model , which allows us to calculate the probability of the unseen sense of the word from a corpus .
 we use the pmtlm of @ cite to learn the predictive word combinations of association rules , which are conceptually similar in spirit to our work .
 we show that this approach can be seen as a special case of the sense that we present in this paper , as we aim to learn a model for each word in the back-off language model .
 in our work , we use them as a baseline to our best-fitting induction problem , which is similar to our approach .

